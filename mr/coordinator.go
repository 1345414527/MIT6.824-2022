package mr

import (
	"errors"
	"log"
	"net"
	"net/http"
	"net/rpc"
	"os"
	"sync"
	"time"
)

type TaskPhase int  //ä»»åŠ¡é˜¶æ®µ
type TaskStatus int //ä»»åŠ¡çŠ¶æ€

//ä»»åŠ¡é˜¶æ®µ
const (
	TaskPhase_Map    TaskPhase = 0
	TaskPhase_Reduce TaskPhase = 1
)

//ä»»åŠ¡çŠ¶æ€
const (
	TaskStatus_New        TaskStatus = 0 //è¿˜æ²¡æœ‰åˆ›å»º
	TaskStatus_Ready      TaskStatus = 1 //è¿›å…¥é˜Ÿåˆ—
	TaskStatus_Running    TaskStatus = 2 //å·²ç»åˆ†é…ï¼Œæ­£åœ¨è¿è¡Œ
	TaskStatus_Terminated TaskStatus = 3 //è¿è¡Œç»“æŸ
	TaskStatus_Error      TaskStatus = 4 //è¿è¡Œå‡ºé”™
)

const (
	ScheduleInterval   = time.Millisecond * 500 //æ‰«æä»»åŠ¡çŠ¶æ€çš„é—´éš”æ—¶é—´
	MaxTaskRunningTime = time.Second * 5        //æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦è¶…æ—¶
)

//ä»»åŠ¡
type Task struct {
	FileName string    //å½“å‰ä»»åŠ¡çš„æ–‡ä»¶å
	Phase    TaskPhase //å½“å‰ä»»åŠ¡çŠ¶æ€
	Seq      int       //å½“å‰çš„ä»»åŠ¡åºåˆ—
	NMap     int       //mapä»»åŠ¡/fileçš„æ•°é‡
	NReduce  int       //reduceä»»åŠ¡/åˆ†åŒºçš„æ•°é‡
	Alive    bool      //æ˜¯å¦å­˜æ´»
}

//ä»»åŠ¡çŠ¶æ€
type TaskState struct {
	Status    TaskStatus //ä»»åŠ¡çŠ¶æ€
	WorkerId  int        //æ‰§è¡Œå½“å‰Taskçš„workerid
	StartTime time.Time  //ä»»åŠ¡å¼€å§‹æ‰§è¡Œçš„æ—¶é—´
}

type Coordinator struct {
	files      []string    //å­˜å‚¨è¦å¤„ç†çš„æ–‡ä»¶
	nReduce    int         //reduce/åˆ†åŒºæ•°é‡
	taskPhase  TaskPhase   //ä»»åŠ¡é˜¶æ®µ
	taskStates []TaskState //ä»»åŠ¡çš„çŠ¶æ€
	taskChan   chan Task   //ä»»åŠ¡é˜Ÿåˆ—
	workerSeq  int         //workeråºåˆ—
	done       bool        //æ˜¯å¦åšå®Œ
	muLock     sync.Mutex  //äº’æ–¥é”

}

//åˆ›å»ºä¸€ä¸ªtask
func (c *Coordinator) NewOneTask(seq int) Task {
	task := Task{
		FileName: "",
		Phase:    c.taskPhase,
		NMap:     len(c.files),
		NReduce:  c.nReduce,
		Seq:      seq,
		Alive:    true,
	}

	DPrintf("m:%+v, taskseq:%d, lenfiles:%d, lents:%d", c, seq, len(c.files), len(c.taskStates))

	if task.Phase == TaskPhase_Map {
		task.FileName = c.files[seq]
	}
	return task
}

//æ‰«æä»»åŠ¡çŠ¶æ€å¹¶é€‚å½“æ›´æ–°
func (c *Coordinator) scanTaskState() {
	DPrintf("scanTaskState...")
	c.muLock.Lock()
	defer c.muLock.Unlock()

	//è¿™é‡Œä¸èƒ½ä½¿ç”¨å‡½æ•°Done()ï¼Œå› ä¸ºæ­¤æ—¶å·²ç»ä¸Šé”
	if c.done {
		return
	}

	allDone := true
	//å¾ªç¯æ¯ä¸ªä»»åŠ¡çš„çŠ¶æ€
	for k, v := range c.taskStates {
		switch v.Status {
		case TaskStatus_New:
			allDone = false
			c.taskStates[k].Status = TaskStatus_Ready
			c.taskChan <- c.NewOneTask(k)
		case TaskStatus_Ready:
			allDone = false
		case TaskStatus_Running:
			allDone = false
			//è¶…æ—¶é‡æ–°åˆ†é…è¯¥ä»»åŠ¡
			if time.Now().Sub(v.StartTime) > MaxTaskRunningTime {
				c.taskStates[k].Status = TaskStatus_Ready
				c.taskChan <- c.NewOneTask(k)
			}
		case TaskStatus_Terminated:
		case TaskStatus_Error:
			allDone = false
			c.taskStates[k].Status = TaskStatus_Ready
			c.taskChan <- c.NewOneTask(k)
		default:
			panic("t. status err in schedule")
		}
	}

	//å¦‚æœå½“å‰ä»»åŠ¡å®Œæˆäº†
	if allDone {
		if c.taskPhase == TaskPhase_Map {
			//è¿›å…¥Reduceé˜¶æ®µ
			DPrintf("init ReduceTask")
			c.taskPhase = TaskPhase_Reduce
			c.taskStates = make([]TaskState, c.nReduce)
		} else {
			log.Println("finish all tasks!!!ğŸ˜Š")
			c.done = true
		}
	}
}

//å®šæ—¶æ›´æ–°çŠ¶æ€
func (c *Coordinator) schedule() {
	for !c.Done() {
		c.scanTaskState()
		time.Sleep(ScheduleInterval)
	}
}

// Your code here -- RPC handlers for the worker to call.

//å¤„ç†Rpcè¯·æ±‚ï¼šè·å–ä»»åŠ¡
func (c *Coordinator) GetOneTask(args *TaskArgs, reply *TaskReply) error {
	task := <-c.taskChan
	reply.Task = &task

	if task.Alive {
		//ä¿®æ”¹çŠ¶æ€
		c.muLock.Lock()
		if task.Phase != c.taskPhase {
			return errors.New("GetOneTask Task phase neq")
		}
		c.taskStates[task.Seq].WorkerId = args.WorkerId
		c.taskStates[task.Seq].Status = TaskStatus_Running
		c.taskStates[task.Seq].StartTime = time.Now()
		c.muLock.Unlock()
	}

	DPrintf("in get one Task, args:%+v, reply:%+v", args, reply)
	return nil
}

//å¤„ç†Rpcè¯·æ±‚ï¼šæ³¨å†Œworker
func (c *Coordinator) RegWorker(args *RegArgs, reply *RegReply) error {
	DPrintf("worker reg!")
	c.muLock.Lock()
	defer c.muLock.Unlock()
	c.workerSeq++
	reply.WorkerId = c.workerSeq
	return nil
}

//å¤„ç†Rpcè¯·æ±‚ï¼šworkerå“åº”taskå®Œæˆæƒ…å†µ
func (c *Coordinator) ReportTask(args *ReportTaskArgs, reply *ReportTaskReply) error {
	c.muLock.Lock()
	defer c.muLock.Unlock()

	DPrintf("get report task: %+v, taskPhase: %+v", args, c.taskPhase)

	//å¦‚æœå‘ç°é˜¶æ®µä¸åŒæˆ–è€…å½“å‰ä»»åŠ¡å·²ç»åˆ†é…ç»™äº†å…¶å®ƒworkerå°±ä¸ä¿®æ”¹å½“å‰ä»»åŠ¡çŠ¶æ€
	if c.taskPhase != args.Phase || c.taskStates[args.Seq].WorkerId != args.WorkerId {
		DPrintf("in report task,workerId=%v report a useless task=%v", args.WorkerId, args.Seq)
		return nil
	}

	if args.Done {
		c.taskStates[args.Seq].Status = TaskStatus_Terminated
	} else {
		c.taskStates[args.Seq].Status = TaskStatus_Error
	}

	go c.scanTaskState()
	return nil
}

//
// an example RPC handler.
//
// the RPC argument and reply types are defined in rpc.go.
//
//func (c *Coordinator) Example(args *ExampleArgs, reply *ExampleReply) error {
//	reply.Y = args.X + 1
//	return nil
//}

//
// start a thread that listens for RPCs from worker.go
//
func (c *Coordinator) server() {
	rpc.Register(c)  // æ³¨å†Œ RPC æœåŠ¡
	rpc.HandleHTTP() // å°† RPC æœåŠ¡ç»‘å®šåˆ° HTTP æœåŠ¡ä¸­å»
	//l, e := net.Listen("tcp", ":1234")
	sockname := coordinatorSock()
	os.Remove(sockname)
	l, e := net.Listen("unix", sockname)
	if e != nil {
		log.Fatal("listen error:", e)
	}
	go http.Serve(l, nil)
}

//
// main/mrcoordinator.go calls Done() periodically to find out
// if the entire job has finished.
//å¦‚æœå·¥ä½œå…¨éƒ¨å®Œæˆï¼Œè¿”å›true
//
func (c *Coordinator) Done() bool {
	c.muLock.Lock()
	defer c.muLock.Unlock()

	return c.done
}

//
// create a Coordinator.
// main/mrcoordinator.go calls this function.
// nReduce is the number of reduce tasks to use.
//
func MakeCoordinator(files []string, nReduce int) *Coordinator {
	c := Coordinator{
		files:      files,
		nReduce:    nReduce,
		taskPhase:  TaskPhase_Map,
		taskStates: make([]TaskState, len(files)),
		workerSeq:  0,
		done:       false,
	}
	if len(files) > nReduce {
		c.taskChan = make(chan Task, len(files))
	} else {
		c.taskChan = make(chan Task, nReduce)
	}

	go c.schedule()
	c.server()
	DPrintf("master init")

	return &c
}
